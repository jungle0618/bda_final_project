convolutional layer 在做影像辨識的時候,還有第三個常用的東西,這個東西叫做pooling。pooling是怎麼來的？pooling來自於另外一個觀察，這個觀察是我們把一張比較大的圖片做subsampling。舉例來說，你把偶數的column都拿掉，基數的row都拿掉，圖片變為原來的四分之一，但是不會影響裡面是什麼東西。把一張大的圖片縮小，這是一隻鳥，這張小圖片，你看起來還是一隻鳥。所以有了pooling這樣的設計，pooling是怎麼運作的？pooling這個東西，它本身沒有參數，所以它不是一個layer，它裡面沒有weight，它沒有要認的東西。所以有人會告訴你說pooling比較像是一個activation function，比較像是sigmoid、ReLU，因為它裡面是沒有要認的東西，它就是一個operator，它的行為都是固定好的，沒有要根據data學任何東西。pooling也有很多不同的版本，我們這邊講的是max pooling。max pooling是怎麼運作的？我們剛才說每一個filter都產生一把數字。要做pooling的時候，我們就把這些數字幾個幾個一組，在這個例子裡面就是二乘二個一組。每一組裡面選一個代表。在max pooling裡面，我們選的代表是最大的那一個。你可能會問說為什麼是選最大的那一個？你不一定要選最大的那一個，這個是你自己可以決定的。max pooling這個方法是選最大的那一個，但是也有min pooling，min pooling是選平均。我還看過選幾何平均的，所以有各式各樣的pooling的方法。你說這邊一定要二乘以二個一組嗎？也不一定，這個也是你自己決定的。你要三乘以三、四乘以四，也可以，這個是你自己決定的。所以我們做完convolution以後，往往後面還會搭配pooling。pooling做的事情是把圖片變小。做完convolution以後我們會得到一張圖片，這張圖片裡面有很多的channel。做完pooling以後我們會把這張圖片的channel不變，本來64個channel還是64個channel，但是我們會把圖片變得比較小張一點。在剛才的例子裡面，本來四乘以四的圖片，如果我們把output的數值二乘以二個一組的話，四乘以四的圖片就會變成二乘以二的圖片，這個是pooling所做的事情。一般在實作上，往往convolution跟pooling交替使用。你可能做幾次convolution，做一次pooling。兩次convolution一次pooling，兩次convolution一次pooling。你可以想見說pooling對於你的performance還是可能會帶來一點傷害的，因為假設你今天要偵測的是非常微細的東西，你隨便做subsampling performance可能會稍微差一點。所以近年來你會發現很多影像辨識的network的設計，往往也開始把pooling丟掉，它會做這種全convolution的neural network，整個network裡面通通都是convolution，完全都不用pooling。這是因為近年來運算能力越來越強，pooling最主要的理由是為了減少運算量，做subsampling，把影像變小，減少運算量。如果你今天你的運算資源足夠支撐你，不做pooling，很多network的架構設計，往往今天就不做pooling，全convolution，convolution從頭到尾，看看做不做的起來，看看能不能夠做得更好。一般你的架構是convolution加pooling。我剛才講過說pooling是可有可無，今天很多人可能會選擇不用pooling。如果你做完幾次convolution以後，接下來最終怎麼得到最後的結果？你會把pooling的output做一件事情叫做flatten。flatten這個字眼剛才在作業二裡面助教也有提到，所謂flatten的意思是把這個影像裡面，本來排成矩陣樣子的東西拉直，把所有的數值拉直變成一個向量。再把這個向量丟進fully connected的layer裡面。最終你可能還要過個softmax，最終得到影像辨識的結果。這是一個經典的影像辨識的network，它可能有的樣子。長這樣，裡面有convolution、有pooling、有flatten，最後再通過幾個fully connected的layer，過softmax，最終得到影像辨識的結果。我們在作業三會做一個影像辨識的題目。除了影像辨識以外，你可能聽過CNN另外一個最常見、最耳熟能詳的應用，是用來下圍棋。今天如果講個機器學習的課沒有提到AlphaGo，大家覺得你什麼都沒講，所以我們來提一下AlphaGo。怎麼用這個CNN來下圍棋？我們說下圍棋是一個分類的問題。你的network的輸入是棋盤上黑子跟白子的位置。你的輸出