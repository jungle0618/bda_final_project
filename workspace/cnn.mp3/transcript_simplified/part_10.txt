不會影響我們對影像中物件的判讀。但是棋盤是這個樣子嗎？你可以把棋盤上的奇數行跟偶數列拿掉，還是同一個棋局嗎？聽起來好像不是，對不對？下圍棋這麼精細的任務，你隨便拿掉一個 column、拿掉一個 row，整個棋局、整個局勢就不一樣了，怎麼可能拿掉一個 row、拿掉一個 column，還會沒有問題呢？

所以有人就會說，CNN 裡面就是要 pooling，影像辨識都是用 pooling，所以 AlphaGo 也一定有 pooling，所以代表 AlphaGo 很爛，針對 pooling 這個弱點去攻擊它，一定就可以把它打爆。真的是這樣嗎？可是 AlphaGo 這麼強，顯然它沒有這麼顯而易見的弱點。所以這個問題就讓我有點困擾。

後來我就去仔細讀了一下 AlphaGo 的 paper，其實 AlphaGo 在 Nature 上的 paper 沒有多長，大概五六頁，你一下子就可以看完了。而且在這個文章的正文裡面，甚至沒有提它用的網路架構是什麼樣的，沒有網路架構的細節。這個細節在哪裡呢？這個細節在附件裡面。所以我就仔細讀了一下附件，看 AlphaGo 的網路結構長什麼樣子。

我們就來看一下這個附件裡面是怎麼描述 AlphaGo 的類神經網路結構的。它先說，我們把一個棋盤看作 19 x 19 x 48 大小的 image。把棋盤看作是一個圖片。他說它有做 zero padding，padding 這件事我們有講，就是你的 filter 如果超出影像的範圍就補零。Zero padding 就是超出範圍就補零的意思。他說它的 filter 大小，kernel size 就是 filter 的大小是 5x5，然後有 K 個 filter。K 是多少？K 是 192。這當然是試出來的，他也試了 128 跟 256，發現 192 最好。這是第一層，stride = 1。stride 是什麼？我們剛才也解釋過了。這邊有用了一個 Rectifier Nonlinearity。這是什麼？這個就是 ReLU。這邊都是 ReLU。

在第二層到第 12 層，都有做 zero padding。這個 kernel size 都是 3x3。一樣是 K 個 filter，也就是每一層都是 192 個 filter。Stride 一樣是 1。這樣疊了很多層以後，因為是一個分類的問題，最後加上了一個 SoftMax。

你有發現什麼玄機嗎？你發現它沒有用 pooling。這給我們一個很好的例子就是，類神經網路的設計，應用之道存乎一心。你不要看影像上面都有用 pooling 就覺得 pooling 一定是好的。在下圍棋的時候就是不適合用 pooling。所以你要想清楚，你今天用一個 network 架構的時候，這個 network 的架構到底代表什麼意思？它是不是適合用在我現在這個任務上？

CNN 除了下圍棋還有影像以外，近年來也用在語音上，也用在文字處理上。這邊我們就不再細講。但是如果你真的想把 CNN 用在語音上，用在文字處理上，你要仔細看一下文獻上的方法。在影像、語音、文字上，receptive field 的設計、參數共享的設計，跟影像上不是一樣的。所以你要想清楚，那些 receptive field 用在語音、文字上的設計跟影像上不是一樣，是考慮了語音跟文字的特性以後所設計的。所以你不要以為在影像上的 CNN 直接套到語音上就可以 work。可能是不 work 的。你要想清楚說影像、語音有什麼樣的特性，那你要怎麼設計合適的 receptive field。

有人會說 CNN 其實沒有辦法處理影像放大縮小或者是旋轉的問題。怎麼說？假設今天你給 CNN 看的狗都是這個大小，它可以辨識說這是一隻狗。當你把這個圖片放大的時候，它可以辨識說它還是一隻狗嗎？可能是不行的。你會想說，怎麼會不能夠辨識？這兩個形狀不是一模一樣，怎麼放大就不能辨識？CNN 這麼笨嗎？它就是這麼笨。對它來說這兩張圖片，雖然形狀一模一樣，但是如果你把它拉長成向量的話，它裡面的數值就是不一樣的。所以對 CNN 來說，雖然你人一看覺得它形狀很像，但對 CNN network 來說，它是非常不一樣的。

所以事實上，CNN 並不能夠處理影像放大縮小或者是旋轉的問題。當它今天在某種大小的影像上，假設你裡面的物件都是比較小的，它在上面學會做影像辨識，你把物件放大它就會整個慘掉。所以 CNN 並沒有你想像的那麼強。那就是為什麼我們在做影像辨識的時候，往往都要做 data augmentation。所以 data augmentation 的意思就是說，你把訓練資料每張圖片截一小塊出來放大，讓 CNN 有看過不同大小的 pattern。或把圖片旋轉，讓它有看過某一個物件旋轉以後長什麼樣子。